{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "038324a0-dad0-4d55-aa83-1bfb46d78c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Mounting your blob storage source and sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f02db1-73f7-42e5-a26d-e226c7e60ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both Blob containers has been mounted\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "storage_account_name = 'testtech'\n",
    "storage_account_key = ''\n",
    "container_name_source = 'bronzelayer'\n",
    "container_name_destination = 'silverlayer'\n",
    "\n",
    "# Configure the spark context with the storage account key\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "# Mount your source container\n",
    "dbutils.fs.mount(\n",
    "  source = f\"wasbs://{container_name_source}@{storage_account_name}.blob.core.windows.net\",\n",
    "  mount_point = f\"/mnt/{container_name_source}\",\n",
    "  extra_configs = {f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_key}\n",
    ")\n",
    "\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = f\"wasbs://{container_name_destination}@{storage_account_name}.blob.core.windows.net\",\n",
    "  mount_point = f\"/mnt/{container_name_destination}\",\n",
    "  extra_configs = {f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_key}\n",
    ")\n",
    "\n",
    "print('both Blob containers has been mounted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12f73869-42d2-48c2-a964-0d476bf68045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unzipping each file in the Bronze Layer container and saving it to the Silver Layer container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0865b876-2971-4ecf-ab60-31727505d688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/mnt/bronzelayer/2009q2.zip', name='2009q2.zip', size=144894, modificationTime=1744090065000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2009q3.zip', name='2009q3.zip', size=3544077, modificationTime=1744090066000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2009q4.zip', name='2009q4.zip', size=4050938, modificationTime=1744090066000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2010q1.zip', name='2010q1.zip', size=5311282, modificationTime=1744090067000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2010q2.zip', name='2010q2.zip', size=3994236, modificationTime=1744090067000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2010q3.zip', name='2010q3.zip', size=13745950, modificationTime=1744090068000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2010q4.zip', name='2010q4.zip', size=14679781, modificationTime=1744090069000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2011q1.zip', name='2011q1.zip', size=19615544, modificationTime=1744090070000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2011q2.zip', name='2011q2.zip', size=15001310, modificationTime=1744090071000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2011q3.zip', name='2011q3.zip', size=57941919, modificationTime=1744090074000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2011q4.zip', name='2011q4.zip', size=64306658, modificationTime=1744090076000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2012q1.zip', name='2012q1.zip', size=85474601, modificationTime=1744090083000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2012q2.zip', name='2012q2.zip', size=80075771, modificationTime=1744090089000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2012q3.zip', name='2012q3.zip', size=89608234, modificationTime=1744090097000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2012q4.zip', name='2012q4.zip', size=95805256, modificationTime=1744090105000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2013q1.zip', name='2013q1.zip', size=99173442, modificationTime=1744090113000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2013q2.zip', name='2013q2.zip', size=96917655, modificationTime=1744090121000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2013q3.zip', name='2013q3.zip', size=98355005, modificationTime=1744090130000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2013q4.zip', name='2013q4.zip', size=92335773, modificationTime=1744090137000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2014q1.zip', name='2014q1.zip', size=102843522, modificationTime=1744090146000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2014q2.zip', name='2014q2.zip', size=85032839, modificationTime=1744090153000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2014q3.zip', name='2014q3.zip', size=87408142, modificationTime=1744090160000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2014q4.zip', name='2014q4.zip', size=87639973, modificationTime=1744090168000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2015q1.zip', name='2015q1.zip', size=98944359, modificationTime=1744090176000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2015q2.zip', name='2015q2.zip', size=78908272, modificationTime=1744090182000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2015q3.zip', name='2015q3.zip', size=82751989, modificationTime=1744090189000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2015q4.zip', name='2015q4.zip', size=83007568, modificationTime=1744090196000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2016q1.zip', name='2016q1.zip', size=95470843, modificationTime=1744090204000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2016q2.zip', name='2016q2.zip', size=73848835, modificationTime=1744090210000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2016q3.zip', name='2016q3.zip', size=71116399, modificationTime=1744090216000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2016q4.zip', name='2016q4.zip', size=79400707, modificationTime=1744090223000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2017q1.zip', name='2017q1.zip', size=92433981, modificationTime=1744090230000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2017q2.zip', name='2017q2.zip', size=71429748, modificationTime=1744090236000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2017q3.zip', name='2017q3.zip', size=74356472, modificationTime=1744090242000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2017q4.zip', name='2017q4.zip', size=76051784, modificationTime=1744090249000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2018q1.zip', name='2018q1.zip', size=93176693, modificationTime=1744090256000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2018q2.zip', name='2018q2.zip', size=91972487, modificationTime=1744090264000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2018q3.zip', name='2018q3.zip', size=87202498, modificationTime=1744090271000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2018q4.zip', name='2018q4.zip', size=62895777, modificationTime=1744090274000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2019q1.zip', name='2019q1.zip', size=95895180, modificationTime=1744090282000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2019q2.zip', name='2019q2.zip', size=90155728, modificationTime=1744090290000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2019q3.zip', name='2019q3.zip', size=89829588, modificationTime=1744090297000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2019q4.zip', name='2019q4.zip', size=98753299, modificationTime=1744090305000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2020q1.zip', name='2020q1.zip', size=96584608, modificationTime=1744090314000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2020q2.zip', name='2020q2.zip', size=82008710, modificationTime=1744090321000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2020q3.zip', name='2020q3.zip', size=87582976, modificationTime=1744090328000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2020q4.zip', name='2020q4.zip', size=89811373, modificationTime=1744090335000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2021q1.zip', name='2021q1.zip', size=97900752, modificationTime=1744090344000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2021q2.zip', name='2021q2.zip', size=90864175, modificationTime=1744090351000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2021q3.zip', name='2021q3.zip', size=97252395, modificationTime=1744090360000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2021q4.zip', name='2021q4.zip', size=102642081, modificationTime=1744090368000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2022q1.zip', name='2022q1.zip', size=103669919, modificationTime=1744090377000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2022q2.zip', name='2022q2.zip', size=97089251, modificationTime=1744090385000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2022q3.zip', name='2022q3.zip', size=97589396, modificationTime=1744090393000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2022q4.zip', name='2022q4.zip', size=111091525, modificationTime=1744090402000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2023q1.zip', name='2023q1.zip', size=113974946, modificationTime=1744090411000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2023q2.zip', name='2023q2.zip', size=116914392, modificationTime=1744090421000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2023q3.zip', name='2023q3.zip', size=117776944, modificationTime=1744090430000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2023q4.zip', name='2023q4.zip', size=120272357, modificationTime=1744090440000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2024q1.zip', name='2024q1.zip', size=124336804, modificationTime=1744152403000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2024q2.zip', name='2024q2.zip', size=119119954, modificationTime=1744152394000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2024q3.zip', name='2024q3.zip', size=118280418, modificationTime=1744152386000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/2024q4.zip', name='2024q4.zip', size=122932548, modificationTime=1744152376000),\n",
       " FileInfo(path='dbfs:/mnt/bronzelayer/temp_folder/', name='temp_folder/', size=0, modificationTime=1744208687000)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(f\"/mnt/{container_name_source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84fab94f-1047-48b6-bc1a-71d8dfaf023c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# List all files in the source container\n",
    "zip_files = dbutils.fs.ls(f\"/mnt/{container_name_source}\")\n",
    "# Filter the list to only include zip files\n",
    "zip_files = [f for f in zip_files if f.name.endswith('.zip')]\n",
    "\n",
    "# Loop through each zip file\n",
    "for zip_file in zip_files:\n",
    "    \n",
    "    # Create a new folder name based on the zip file name\n",
    "    new_folder_name = zip_file.name.split('.')[0]\n",
    "    # Create a new directory in the destination container\n",
    "    dbutils.fs.mkdirs(f\"/mnt/{container_name_destination}/{new_folder_name}\")\n",
    "    \n",
    "    # Define the temporary extraction path\n",
    "    extract_path = f'/dbfs/mnt/{container_name_source}/temp_folder'\n",
    "    # Define the working directory where the zip file is located\n",
    "    working_dir = f'/dbfs/mnt/{container_name_source}/{new_folder_name}.zip'\n",
    "    # Create the temporary extraction directory\n",
    "    dbutils.fs.mkdirs(extract_path)\n",
    "    \n",
    "    # Open the zip file and extract its contents to the temporary directory\n",
    "    with zipfile.ZipFile(working_dir, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    \n",
    "    # Define the path to the 'sub.txt' file inside the temporary directory\n",
    "    sub_path = f'/mnt/{container_name_source}/temp_folder/sub.txt'\n",
    "    # Read the 'sub.txt' file into a DataFrame with specified delimiter and header\n",
    "    sub_df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(sub_path)\n",
    "    # Convert the 'filed' and 'period' columns to date format\n",
    "    sub_df = sub_df.withColumn(\"filed\", to_date(col(\"filed\"), \"yyyyMMdd\")).withColumn(\"period\", to_date(col(\"period\"), \"yyyyMMdd\"))\n",
    "    # Write the DataFrame to a Parquet file in the destination directory\n",
    "    sub_df.write.mode(\"overwrite\").parquet(f'/mnt/{container_name_destination}/{new_folder_name}/sub.parquet')\n",
    "    \n",
    "    # Define the path to the 'tag.txt' file inside the temporary directory\n",
    "    tag_path = f'/mnt/{container_name_source}/temp_folder/tag.txt'\n",
    "    # Read the 'tag.txt' file into a DataFrame with specified delimiter and header\n",
    "    tag_df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(tag_path)\n",
    "    # Write the DataFrame to a Parquet file in the destination directory\n",
    "    tag_df.write.mode(\"overwrite\").parquet(f'/mnt/{container_name_destination}/{new_folder_name}/tag.parquet')\n",
    "    \n",
    "    # Define the path to the 'num.txt' file inside the temporary directory\n",
    "    num_path = f'/mnt/{container_name_source}/temp_folder/num.txt'\n",
    "    # Read the 'num.txt' file into a DataFrame with specified delimiter and header\n",
    "    num_df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(num_path)\n",
    "    num_df = num_df.withColumn(\"ddate\", to_date(col(\"ddate\"), \"yyyyMMdd\"))\n",
    "    # Write the DataFrame to a Parquet file in the destination directory\n",
    "    num_df.write.mode(\"overwrite\").parquet(f'/mnt/{container_name_destination}/{new_folder_name}/num.parquet')\n",
    "    \n",
    "    # Define the path to the 'pre.txt' file inside the temporary directory\n",
    "    pre_path = f'/mnt/{container_name_source}/temp_folder/pre.txt'\n",
    "    # Read the 'pre.txt' file into a DataFrame with specified delimiter and header\n",
    "    pre_df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(pre_path)\n",
    "    # Write the DataFrame to a Parquet file in the destination directory\n",
    "    pre_df.write.mode(\"overwrite\").parquet(f'/mnt/{container_name_destination}/{new_folder_name}/pre.parquet')\n",
    "    \n",
    "    # Remove the temporary extraction directory and its contents\n",
    "    dbutils.fs.rm(extract_path, recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9260131-8ea4-434d-96e1-b4daa56f4a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save as CSV to the target container (OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ec82103-bea3-4f89-bd58-a9aaef49c96f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# List all files in the source container\n",
    "zip_files = dbutils.fs.ls(f\"/mnt/{container_name_source}\")\n",
    "\n",
    "# Filter the list to only include zip files\n",
    "zip_files = [f for f in zip_files if f.name.endswith('.zip')]\n",
    "\n",
    "# Loop through each zip file\n",
    "for zip_file in zip_files:\n",
    "    # Create a new folder name based on the zip file name\n",
    "    new_folder_name = zip_file.name.split('.')[0]\n",
    "    \n",
    "    # Create a new directory in the destination container\n",
    "    dbutils.fs.mkdirs(f\"/mnt/{container_name_destination}/{new_folder_name}\")\n",
    "    \n",
    "    # Define the temporary extraction path\n",
    "    extract_path = f'/dbfs/mnt/{container_name_source}/temp_folder'\n",
    "    \n",
    "    # Define the working directory where the zip file is located\n",
    "    working_dir = f'/dbfs/mnt/{container_name_source}/{new_folder_name}.zip'\n",
    "    \n",
    "    # Create the temporary extraction directory\n",
    "    dbutils.fs.mkdirs(extract_path)\n",
    "    \n",
    "    # Open the zip file and extract its contents to the temporary directory\n",
    "    with zipfile.ZipFile(working_dir, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    \n",
    "    # Define the path to the 'sub.txt' file inside the temporary directory\n",
    "    sub_path = f'/mnt/{container_name_source}/temp_folder/sub.txt'\n",
    "    \n",
    "    # Read the 'sub.txt' file into a DataFrame with specified delimiter and header\n",
    "    sub_df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(sub_path)\n",
    "    \n",
    "    # Convert the 'filed' and 'period' columns to date format\n",
    "    sub_df = sub_df.withColumn(\"filed\", to_date(col(\"filed\"), \"yyyyMMdd\")).withColumn(\"period\", to_date(col(\"period\"), \"yyyyMMdd\"))\n",
    "    \n",
    "    # Write the DataFrame to a CSV file in the destination directory\n",
    "    sub_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f'/mnt/{container_name_destination}/{new_folder_name}/sub.csv')\n",
    "    \n",
    "    # Define the path to the 'tag.txt' file inside the temporary directory\n",
    "    tag_path = f'/mnt/{container_name_source}/temp_folder/tag.txt'\n",
    "    \n",
    "    # Read the 'tag.txt' file into a DataFrame with specified delimiter and header\n",
    "    tag_df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(tag_path)\n",
    "    \n",
    "    # Write the DataFrame to a CSV file in the destination directory\n",
    "    tag_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f'/mnt/{container_name_destination}/{new_folder_name}/tag.csv')\n",
    "    \n",
    "    # Define the path to the 'num.txt' file inside the temporary directory\n",
    "    num_path = f'/mnt/{container_name_source}/temp_folder/num.txt'\n",
    "    \n",
    "    # Read the 'num.txt' file into a DataFrame with specified delimiter and header\n",
    "    num_df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(num_path)\n",
    "    num_df = num_df.withColumn(\"ddate\", to_date(col(\"ddate\"), \"yyyyMMdd\"))\n",
    "    \n",
    "    # Write the DataFrame to a CSV file in the destination directory\n",
    "    num_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f'/mnt/{container_name_destination}/{new_folder_name}/num.csv')\n",
    "    \n",
    "    # Define the path to the 'pre.txt' file inside the temporary directory\n",
    "    pre_path = f'/mnt/{container_name_source}/temp_folder/pre.txt'\n",
    "    \n",
    "    # Read the 'pre.txt' file into a DataFrame with specified delimiter and header\n",
    "    pre_df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(pre_path)\n",
    "    \n",
    "    # Write the DataFrame to a CSV file in the destination directory\n",
    "    pre_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f'/mnt/{container_name_destination}/{new_folder_name}/pre.csv')\n",
    "    \n",
    "    # Remove the temporary extraction directory and its contents\n",
    "    dbutils.fs.rm(extract_path, recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be31d2ce-8535-4362-9b82-e9463a4a310c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unmounting the blob storage containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e26a983-2d48-486c-bd67-728e6d86f17b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/bronzelayer has been unmounted.\n",
      "/mnt/silverlayer has been unmounted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unmount the source container\n",
    "dbutils.fs.unmount(f\"/mnt/{container_name_source}\")\n",
    "# Unmount the destination container\n",
    "dbutils.fs.unmount(f\"/mnt/{container_name_destination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfab4dc5-6b2d-4fec-8350-23195e83a184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating Delta Tables for our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7347c9ca-b14f-41b7-a0c3-c70f9a88d8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Submissions table\n",
    "CREATE TABLE Submissions (\n",
    "    adsh STRING NOT NULL,\n",
    "    cik BIGINT,\n",
    "    name STRING NOT NULL,\n",
    "    sic INT,\n",
    "    countryba STRING,\n",
    "    stprba STRING,\n",
    "    cityba STRING,\n",
    "    zipba STRING,\n",
    "    bas1 STRING,\n",
    "    bas2 STRING,\n",
    "    baph STRING,\n",
    "    countryma STRING,\n",
    "    stprma STRING,\n",
    "    cityma STRING,\n",
    "    zipma STRING,\n",
    "    mas1 STRING,\n",
    "    mas2 STRING,\n",
    "    countryinc STRING ,\n",
    "    stprinc STRING,\n",
    "    ein STRING,\n",
    "    former STRING,\n",
    "    changed DATE,\n",
    "    afs STRING,\n",
    "    wksi BOOLEAN NOT NULL,\n",
    "    fye STRING,\n",
    "    form STRING,\n",
    "    period DATE ,\n",
    "    fy INT ,\n",
    "    fp STRING,\n",
    "    filed DATE NOT NULL,\n",
    "    accepted TIMESTAMP NOT NULL,\n",
    "    prevrpt BOOLEAN NOT NULL,\n",
    "    detail BOOLEAN NOT NULL,\n",
    "    instance STRING NOT NULL,\n",
    "    nciks INT,\n",
    "    aciks STRING\n",
    ") USING DELTA;\n",
    "\n",
    "-- Tags table\n",
    "CREATE TABLE Tags (\n",
    "    tag STRING NOT NULL,\n",
    "    version STRING NOT NULL,\n",
    "    custom INT NOT NULL,\n",
    "    abstract BOOLEAN NOT NULL,\n",
    "    datatype STRING,\n",
    "    iord STRING,\n",
    "    crdr STRING,\n",
    "    tlabel STRING,\n",
    "    doc STRING\n",
    ") USING DELTA;\n",
    "\n",
    "-- Numbers table\n",
    "CREATE TABLE Numbers (\n",
    "    adsh STRING NOT NULL,\n",
    "    tag STRING NOT NULL,\n",
    "    version STRING NOT NULL,\n",
    "    ddate DATE NOT NULL,\n",
    "    qtrs INT NOT NULL,\n",
    "    uom STRING NOT NULL,\n",
    "    coreg STRING ,\n",
    "    value DECIMAL(28,4),\n",
    "    footnote STRING\n",
    ") USING DELTA;\n",
    "\n",
    "-- Presentations table\n",
    "CREATE TABLE Presentations (\n",
    "    adsh STRING NOT NULL,\n",
    "    report INT NOT NULL,\n",
    "    line INT NOT NULL,\n",
    "    stmt STRING,\n",
    "    inpth BOOLEAN NOT NULL,\n",
    "    rfile STRING,\n",
    "    tag STRING,\n",
    "    version STRING,\n",
    "    plabel STRING\n",
    ") USING DELTA;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e107e5f-f5a0-4301-a173-f95516ad5533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Adding constraints and Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc7bb32-44af-42f1-b5be-9a0612b74e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Add primary keys\n",
    "ALTER TABLE Submissions ADD CONSTRAINT pk_submissions PRIMARY KEY (adsh);\n",
    "ALTER TABLE Tags ADD CONSTRAINT pk_tags PRIMARY KEY (tag, version);\n",
    "ALTER TABLE Numbers ADD CONSTRAINT pk_numbers PRIMARY KEY (adsh, tag, version, ddate, qtrs, uom);\n",
    "ALTER TABLE Presentations ADD CONSTRAINT pk_presentations PRIMARY KEY (adsh, report, line);\n",
    "\n",
    "-- Add foreign keys\n",
    "ALTER TABLE Numbers ADD CONSTRAINT fk_numbers_submissions \n",
    "    FOREIGN KEY (adsh) REFERENCES Submissions(adsh);\n",
    "ALTER TABLE Numbers ADD CONSTRAINT fk_numbers_tags \n",
    "    FOREIGN KEY (tag, version) REFERENCES Tags(tag, version);\n",
    "ALTER TABLE Presentations ADD CONSTRAINT fk_presentations_submissions \n",
    "    FOREIGN KEY (adsh) REFERENCES Submissions(adsh);\n",
    "ALTER TABLE Presentations ADD CONSTRAINT fk_presentations_tags \n",
    "    FOREIGN KEY (tag, version) REFERENCES Tags(tag, version);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5947e721-5c27-455c-aab7-c5df0d812b36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Moving the unzipped Parquet files to the delta table\n",
    "## Mount the container with the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e37c3cb2-0200-465d-8e7f-c481580bedb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destination container has been mounted\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "storage_account_name = 'testtech'\n",
    "storage_account_key = ''\n",
    "container_name_destination = 'silverlayer'\n",
    "\n",
    "# Configure the spark context with the storage account key\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = f\"wasbs://{container_name_destination}@{storage_account_name}.blob.core.windows.net\",\n",
    "  mount_point = f\"/mnt/{container_name_destination}\",\n",
    "  extra_configs = {f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_key}\n",
    ")\n",
    "\n",
    "print('Destination container has been mounted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da6b3151-01ac-4836-a979-12435c4d2e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Enforce Database Schema Push to Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "485ed83e-8dcd-4482-a1bf-2f22a97d4c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"ParquetToDelta\").getOrCreate()\n",
    "\n",
    "# Define the root directory where your Blob Storage is mounted\n",
    "root_directory = f\"/mnt/{container_name_destination}\"\n",
    "\n",
    "# Define schemas for each table\n",
    "submissions_schema = StructType([\n",
    "    StructField(\"adsh\", StringType(), False), # Not nullable\n",
    "    StructField(\"cik\", LongType(), True), # Nullable\n",
    "    StructField(\"name\", StringType(), True), # Nullable\n",
    "    StructField(\"sic\", IntegerType(), True), # Nullable\n",
    "    StructField(\"countryba\", StringType(), False), # Not nullable\n",
    "    StructField(\"stprba\", StringType(), True), # Nullable\n",
    "    StructField(\"cityba\", StringType(), False), # Not nullable\n",
    "    StructField(\"zipba\", StringType(), True), # Nullable\n",
    "    StructField(\"bas1\", StringType(), True), # Nullable\n",
    "    StructField(\"bas2\", StringType(), True), # Nullable\n",
    "    StructField(\"baph\", StringType(), True), # Nullable\n",
    "    StructField(\"countryma\", StringType(), True), # Nullable\n",
    "    StructField(\"stprma\", StringType(), True), # Nullable\n",
    "    StructField(\"cityma\", StringType(), True), # Nullable\n",
    "    StructField(\"zipma\", StringType(), True), # Nullable\n",
    "    StructField(\"mas1\", StringType(), True), # Nullable\n",
    "    StructField(\"mas2\", StringType(), True), # Nullable\n",
    "    StructField(\"countryinc\", StringType(), False), # Not nullable\n",
    "    StructField(\"stprinc\", StringType(), True), # Nullable\n",
    "    StructField(\"ein\", StringType(), True), # Nullable\n",
    "    StructField(\"former\", StringType(), True), # Nullable\n",
    "    StructField(\"changed\", DateType(), True), # Nullable\n",
    "    StructField(\"afs\", StringType(), True), # Nullable\n",
    "    StructField(\"wksi\", BooleanType(), False), # Not nullable\n",
    "    StructField(\"fye\", StringType(), False), # Not nullable\n",
    "    StructField(\"form\", StringType(), False), # Not nullable\n",
    "    StructField(\"period\", DateType(), True), # Not nullable\n",
    "    StructField(\"fy\", IntegerType(), False), # Not nullable\n",
    "    StructField(\"fp\", StringType(), False), # Not nullable\n",
    "    StructField(\"filed\", DateType(), False), # Not nullable\n",
    "    StructField(\"accepted\", TimestampType(), False), # Not nullable\n",
    "    StructField(\"prevrpt\", BooleanType(), False), # Not nullable\n",
    "    StructField(\"detail\", BooleanType(), False), # Not nullable\n",
    "    StructField(\"instance\", StringType(), False), # Not nullable\n",
    "    StructField(\"nciks\", IntegerType(), False), # Not nullable\n",
    "    StructField(\"aciks\", StringType(), True) # Nullable\n",
    "])\n",
    "\n",
    "tags_schema = StructType([\n",
    "    StructField(\"tag\", StringType(), False), # Not nullable\n",
    "    StructField(\"version\", StringType(), False), # Not nullable\n",
    "    StructField(\"custom\", IntegerType(), False), # Not nullable\n",
    "    StructField(\"abstract\", BooleanType(), False), # Not nullable\n",
    "    StructField(\"datatype\", StringType(), True), # Nullable\n",
    "    StructField(\"iord\", StringType(), True), # Nullable\n",
    "    StructField(\"crdr\", StringType(), True), # Nullable\n",
    "    StructField(\"tlabel\", StringType(), True), # Nullable\n",
    "    StructField(\"doc\", StringType(), True) # Nullable\n",
    "])\n",
    "\n",
    "numbers_schema = StructType([\n",
    "    StructField(\"adsh\", StringType(), True), # Nullable\n",
    "    StructField(\"tag\", StringType(), True), # Nullable\n",
    "    StructField(\"version\", StringType(), True), # Nullable\n",
    "    StructField(\"ddate\", DateType(), True), # Nullable\n",
    "    StructField(\"qtrs\", IntegerType(), True), # Nullable\n",
    "    StructField(\"uom\", StringType(), True), # Nullable\n",
    "    StructField(\"coreg\", StringType(), True), # Nullable\n",
    "    StructField(\"value\", DecimalType(28,4), True), # Nullable\n",
    "    StructField(\"footnote\", StringType(), True) # Nullable\n",
    "])\n",
    "\n",
    "presentations_schema = StructType([\n",
    "    StructField(\"adsh\", StringType(), False), # Not nullable\n",
    "    StructField(\"report\", IntegerType(), True), # Nullable\n",
    "    StructField(\"line\", IntegerType(), False), # Not nullable\n",
    "    StructField(\"stmt\", StringType(), False), # Not nullable\n",
    "    StructField(\"inpth\", BooleanType(), False), # Not nullable\n",
    "    StructField(\"rfile\", StringType(), False), # Not nullable\n",
    "    StructField(\"tag\", StringType(), False), # Not nullable\n",
    "    StructField(\"version\", StringType(), False), # Not nullable\n",
    "    StructField(\"plabel\", StringType(), False) # Not nullable\n",
    "])\n",
    "\n",
    "\n",
    "# Function to process files in a specific quarter folder\n",
    "def process_quarter_folder(folder_path):\n",
    "    files = dbutils.fs.ls(folder_path) # List all files in the folder\n",
    "    for file in files:\n",
    "        if file.name == 'pre.parquet/':\n",
    "            file_path = file.path\n",
    "            df = spark.read.parquet(file_path) # Read parquet file\n",
    "            for field in presentations_schema.fields:\n",
    "                df = df.withColumn(field.name, col(field.name).cast(field.dataType)) # Cast columns to the defined schema\n",
    "            df.write.mode(\"append\").saveAsTable('Presentations') # Save to Delta table\n",
    "        elif file.name == 'num.parquet/':\n",
    "            file_path = file.path\n",
    "            df = spark.read.parquet(file_path)\n",
    "            for field in numbers_schema.fields:\n",
    "                df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "            df.write.mode(\"append\").saveAsTable('Numbers')\n",
    "        elif file.name == 'sub.parquet/':\n",
    "            file_path = file.path\n",
    "            df = spark.read.parquet(file_path)\n",
    "            for field in submissions_schema.fields:\n",
    "                df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "            df.write.mode(\"append\").saveAsTable('Submissions')\n",
    "        elif file.name == 'tag.parquet/':\n",
    "            file_path = file.path\n",
    "            df = spark.read.parquet(file_path)\n",
    "            for field in tags_schema.fields:\n",
    "                df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "            df.write.mode(\"append\").saveAsTable('Tags')\n",
    "        else:\n",
    "            print(f\"Skipping file {file.name} as it doesn't match any known pattern\")\n",
    "\n",
    "# Function to process all quarter folders\n",
    "def process_all_quarters():\n",
    "    quarters = dbutils.fs.ls(root_directory) # List all quarter folders\n",
    "    for quarter in quarters:\n",
    "        print(f\"Processing folder: {quarter.name}\")\n",
    "        process_quarter_folder(quarter.path) # Process each quarter folder\n",
    "\n",
    "# Enable automatic schema merging in Delta Lake\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "print(f\"Starting to process all quarter folders from {root_directory}\")\n",
    "process_all_quarters()\n",
    "\n",
    "print(\"Data upload completed for all quarters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdf84a4e-6008-4a87-8210-44b1e8e74d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from tags;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1bc01d2-78d4-47b2-b4a0-85709f01d789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.unmount(f\"/mnt/{container_name_destination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "113bc429-5e8d-44bb-be3d-b837431af6b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Database tables in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97517e34-348a-423c-801d-ee77842506b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "#https://www.youtube.com/watch?v=9PBvVeCQi0w WATCH THIS\n",
    "#https://www.youtube.com/watch?v=VOJ54hu2e2Q\n",
    "#https://www.youtube.com/watch?v=CUR6rKrIEGc\n",
    "\n",
    "USE DATABASE FINANCIAL;\n",
    "USE SCHEMA ALLL;\n",
    "\n",
    "CREATE TABLE Submissions (\n",
    "    adsh TEXT PRIMARY KEY,\n",
    "    cik BIGINT,\n",
    "    name TEXT NOT NULL,\n",
    "    sic INT,\n",
    "    countryba TEXT,\n",
    "    stprba TEXT,\n",
    "    cityba TEXT,\n",
    "    zipba TEXT,\n",
    "    bas1 TEXT,\n",
    "    bas2 TEXT,\n",
    "    baph TEXT,\n",
    "    countryma TEXT,\n",
    "    stprma TEXT,\n",
    "    cityma TEXT,\n",
    "    zipma TEXT,\n",
    "    mas1 TEXT,\n",
    "    mas2 TEXT,\n",
    "    countryinc TEXT,\n",
    "    stprinc TEXT,\n",
    "    ein TEXT,\n",
    "    former TEXT,\n",
    "    changed DATE,\n",
    "    afs TEXT,\n",
    "    wksi BOOLEAN NOT NULL,\n",
    "    fye TEXT,\n",
    "    form TEXT,\n",
    "    period DATE,\n",
    "    fy INT,\n",
    "    fp TEXT,\n",
    "    filed DATE NOT NULL,\n",
    "    accepted TIMESTAMP NOT NULL,\n",
    "    prevrpt BOOLEAN NOT NULL,\n",
    "    detail BOOLEAN NOT NULL,\n",
    "    instance TEXT NOT NULL,\n",
    "    nciks INT,\n",
    "    aciks TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE Tags (\n",
    "    tag TEXT NOT NULL,\n",
    "    version TEXT NOT NULL,\n",
    "    custom INT NOT NULL,\n",
    "    abstract BOOLEAN NOT NULL,\n",
    "    datatype TEXT,\n",
    "    iord TEXT,\n",
    "    crdr TEXT,\n",
    "    tlabel TEXT,\n",
    "    doc TEXT,\n",
    "    PRIMARY KEY (tag, version)\n",
    ");\n",
    "\n",
    "CREATE TABLE Numbers (\n",
    "    adsh TEXT NOT NULL,\n",
    "    tag TEXT NOT NULL,\n",
    "    version TEXT NOT NULL,\n",
    "    ddate DATE NOT NULL,\n",
    "    qtrs INT NOT NULL,\n",
    "    uom TEXT NOT NULL,\n",
    "    coreg TEXT,\n",
    "    value NUMERIC(28,4),\n",
    "    footnote TEXT,\n",
    "    PRIMARY KEY (adsh, tag, version, ddate, qtrs, uom),\n",
    "    FOREIGN KEY (adsh) REFERENCES Submissions(adsh),\n",
    "    FOREIGN KEY (tag, version) REFERENCES Tags(tag, version)\n",
    ");\n",
    "\n",
    "CREATE TABLE Presentations (\n",
    "    adsh TEXT NOT NULL,\n",
    "    report INT NOT NULL,\n",
    "    line INT NOT NULL,\n",
    "    stmt TEXT,\n",
    "    inpth BOOLEAN NOT NULL,\n",
    "    rfile TEXT,\n",
    "    tag TEXT,\n",
    "    version TEXT,\n",
    "    plabel TEXT,\n",
    "    PRIMARY KEY (adsh, report, line),\n",
    "    FOREIGN KEY (adsh) REFERENCES Submissions(adsh),\n",
    "    FOREIGN KEY (tag, version) REFERENCES Tags(tag, version)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4526a0fc-7b32-4d8c-a2fd-773cb5cadb75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Push to SNOWFLAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "369190f8-329a-4d20-a020-9ee230c95a73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sfOptions = {\n",
    "    \"sfURL\": \"MPAGWCW-OX41008.snowflakecomputing.com\",\n",
    "    \"sfDatabase\": \"FINANCIAL\",\n",
    "    \"sfSchema\": \"ALLL\",\n",
    "    \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "    \"sfUser\": \"MICHAELGEORGE\",\n",
    "    \"sfPassword\": \"\"\n",
    "}\n",
    "\n",
    "sub = spark.read.format(\"delta\").table(\"Submissions\")\n",
    "tag = spark.read.format(\"delta\").table(\"Tags\")\n",
    "pre = spark.read.format(\"delta\").table(\"Presentations\")\n",
    "num = spark.read.format(\"delta\").table(\"Numbers\")\n",
    "\n",
    "\n",
    "\n",
    "sub.write \\\n",
    "  .format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"Submissions\").mode(\"overwrite\").save()\n",
    "\n",
    "tag.write \\\n",
    "  .format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"Tags\").mode(\"overwrite\").save()\n",
    "\n",
    "\n",
    "pre.write \\\n",
    "  .format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"Presentations\").mode(\"overwrite\").save()\n",
    "\n",
    "num.write \\\n",
    "  .format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"Numbers\").mode(\"overwrite\").save()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8629520056423783,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Medallion push all",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
